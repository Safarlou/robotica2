\documentclass[10pt,twocolumn]{scrartcl} 
\usepackage{simpleConference}
\usepackage{customTikz}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{url}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{gensymb}
\usepackage{varioref}
\usepackage[all]{hypcap}
\usepackage{subcaption}

\hypersetup{
  colorlinks=true,
  linkcolor=black,
  urlcolor=black,
  citecolor=black
}

\usetikzlibrary{calc}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\DeclareMathOperator{\dist}{d}
\DeclareMathOperator{\proj}{\phi}

\newcommand{\fref}[1]{\hyperref[#1]{figure \vref{#1}}}
\newcommand{\sref}[1]{section \vref{#1}: \nameref{#1}}
\newcommand{\link}[2]{\textsc{\href{#1}{#2}}}

\begin{document}

\title{CONVOI Project Report \\ \vspace{.5em} \normalsize{Camera-Operated Navigation and Vehicle-Object Interaction} }

\author{Koen Dercksen \hspace{2em} Marein K\"onings\\
Caspar Safarlou \hspace{2em} Chris Kamphuis \hspace{2em} Erik Verboom\\
\\
January 2014 \\
\\
for the course of Robotica II\\
taught by Perry Groot and Ida Sprinkhuizen-Kuyper\\
assisted by Bas Bootsma \\
\\
as part of Artificial Intelligence\\
at Radboud University, Nijmegen
}

\maketitle
\thispagestyle{empty} % remove page number on this page

\begin{abstract}
   This is a simple sample of a document created using \LaTeX
   (specifically pdflatex)
   that includes a figure from the Vergil visual editor for Ptolemy II
   that was created by printing to the Acrobat Distiller to get a PDF file.
   It also illustrates a simple two-column conference paper style,
   and use of bibtex to handle bibligraphies.
\end{abstract}

\tableofcontents

\section{Project Outline}

\subsection{The Assignment}

\subsection{Task and Execution}
What the robot does, brief description of steps involved. Intro to sections about code and physical.

\subsection{Development Process}
How the project went, timeline, overview of problems and solutions. Intro to sections about unimplemented features and team.

\section{Third-Party Software}

\subsection{Development Tools}
Overview of software used for development (language, IDE, VCS).

\subsubsection{C\#}
including: what is it, why we chose it

\subsubsection{Visual Studio}
including: what is it, why we chose it (name cool features like performance analysis...)

\subsubsection{GIT}
including: what is it, why we chose it

\subsubsection{Miscellaneous}
Photoshop...

\subsection{Libraries and Algorithms}
As part of our project, we used several third-party libraries in our code to solve certain problems. We also made use of (mathematical) algorithms from online sources. All of these are listed below, including our reasoning for using the software rather than writing our own. In most of these cases, a primary factor was a need not to 'reinvent the wheel'. Of course we only applied this reasoning to problems that we didn't find interesting, as we did write our own code in other areas of the project. We would like to note that we have solid ideas on how we would ourselves solve most of the problems below.

\subsubsection{Logitech Camera Software}
The webcam we used, the Logitech QuickCam Pro 9000, comes with specialized software that includes the ability to set such properties as zoom, focus, contrast and color intensity. We use this software to pre-process the webcam input, removing noise and turning it into something that is more easily analysed by our software. The effect of pre-processing can be seen in \fref{fig:preprocessing}.

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\includegraphics[width=\columnwidth]{pictures/preprocessing1.png}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\includegraphics[width=\columnwidth]{pictures/preprocessing2.png}
	\end{subfigure}
	\caption{Webcam input before and after pre-processing.}
	\label{fig:preprocessing}
\end{figure}

We were initially unaware of the Logitech software, and considered writing our own code for pre-processing. We decided this was too much effort compared to the gain, especially since our analysis software was able to handle input without pre-processing. When we discovered the Logitech software and the ease with which is allowed us to pre-process the input, we decided to incorporate it into our analysis pipeline to further decrease errors.

\subsubsection{EmguCV}
\link{http://www.emgu.com/wiki/index.php/Main_Page}{EmguCV} is a .NET wrapper of OpenCV, an image processing library developed by Intel. It offers a wide range of functionalities from image and motion detection to machine learning algorithms. We only utilize a very small sample from these functionalities.

\paragraph{Camera Connection}
The wrapper contains methods for very easily communicating with a connected camera. Only a couple of lines of code are needed to establish a streaming input connection. We didn't anticipate EmguCV to contain this functionality, but we were very glad to discover it as it exactly covered our needs.

We also set up a 'mock' input stream, which supplied pre-determined images to the analysis pipeline. This mock input stream proved very useful for testing purposes, especially at times when we didn't have access to the robots or camera.

\paragraph{Contour Detection}
An important step in our analysis is the conversion of blobs to contours describing the shape of the blobs. Blobs are defined by a bitmap, with bits representing one of two values, in this way defining one or multiple contiguous regions known as blobs. Contours are defined here as polygons that (roughly) describe the shape of blobs. The important thing is that blob geometry is converted from a bitmap representation to an approximated vector representation. The result of the process can be seen in \fref{fig:contourDetection}.

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\includegraphics[width=\columnwidth]{pictures/contourDetection1.png}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\includegraphics[width=\columnwidth]{pictures/contourDetection2.png}
	\end{subfigure}
	\caption{Blobs and corresponding contours.}
	\label{fig:contourDetection}
\end{figure}

EmguCV contains functionality to do exactly this, allowing some control over the algorithm used. We found the contour-finding algorithm known in EmguCV as LINK\_RUNS led to the best results in our situation. Unfortunately we found no documentation on the inner workings of the algorithm, and \link{https://github.com/Itseez/opencv/blob/bad927325fc9b73ad449fd46f8856a2cea448390/modules/imgproc/src/contours.cpp\#L129}{the source code} does not grant much insight.

\paragraph{Minimum Area Rectangle}
Many of the items we are detecting are rectangular, but may not appear exactly rectangular in the input (due to perspective and colour matching inaccuracies). As such, it is convenient to have a method of easily converting an arbitrary contour, which we have already established to represent a rectangular item, to a rectangular contour. EmguCV's 'minimum area rectangle' (MinAreaRect) functionality solves this problem for us.

Much like the common principle of a 'bounding box', the MinAreaRect is a rectangular box around a collection of points, such that the box contains all points, and the box could not be any smaller while still containing all points. Contrary to a regular bounding box, a MinAreaRect is not necessarily aligned to the Cartesian axes. This means that the MinAreaRect of a particular contour usually has a smaller area than the bounding box of that same contour, as the rotation of the rectangle allows for a more optimal fit. The principles of bounding boxes and minimum area rectangles are illustrated in \fref{fig:minAreaRect}.

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.30\columnwidth}
	\centering
	\input{pictures/minAreaRect1.tex}
	\subcaption{A contour.}
	\end{subfigure}
	\begin{subfigure}[t]{0.30\columnwidth}
	\centering
	\input{pictures/minAreaRect2.tex}
	\subcaption{The bounding box of the contour, with an area of 10 units$^2$.}
	\end{subfigure}
	\begin{subfigure}[t]{0.30\columnwidth}
	\centering
	\input{pictures/minAreaRect3.tex}
	\subcaption{The minimum area rectangle of the contour, with an area of 7 units$^2$.}
	\end{subfigure}
	\caption{Illustration of bounding box and minimum area rectangle.}
	\label{fig:minAreaRect}
\end{figure}

In our implementation, with items that we know to be rectangles, the MinAreaRect is simply taken as the actual shape of the item. We were again unable to find information on the algorithm used.

\paragraph{Convex Hull}
The convex hull of a set of points is the smallest subset that defines a convex polygon, such that the polygon contains all of the original points. Practically, the original set of points may be seen as a polygon, and the convex hull is a modification of that polygon with all concavities removed, resulting in a convex polygon. An example of convex hull generation can be seen in \fref{fig:convexHull}.

\begin{figure}
	\centering
	\input{pictures/convexHull.tex}
	\caption{\small Convex hull of the polygon seen in \fref{fig:minAreaRect}.}
	\label{fig:convexHull}
\end{figure}

We apply EmguCV's method for determining the convex hull of a polygon on contours, when we are looking for items that we know to have a convex shape. This accounts for some errors in colour matching, where part of an object was not detected as belonging to the object, resulting in a concave shape. This concave shape is made convex by converting to convex hull, removing the concavity that was missed in detection. Documentation on the algorithm is again lacking (\link{https://github.com/Itseez/opencv/blob/ef91d7e8830c36785f0b6fdbf2045da48413dd76/modules/imgproc/src/convhull.cpp\#L129}{source code}).

\paragraph{EmguCV Representation}
EmguCV uses its own data formats for almost all of its functionality. Since we are using this functionality in key areas of our pipeline, we came to use those data formats in many places ourselves. This includes the representation of polygons and images, and all associated representations such as points and colours.

However, there are two problems with this established setup, both to do with performance issues. These are listed in \sref{sec:emgucvFormats}, as there were plans for a better solution.

\subsubsection{Triangle.NET}
\link{http://triangle.codeplex.com/}{Triangle.NET} is a library that facilitates (Delauney) triangulation. Triangulation is the process of taking a polygon and dividing it up into triangles. This technique is a key part of our navigation mesh construction. Because of the key role of triangulation, and because we found it to be a difficult problem, and such a perfectly-suited library exists, we chose to use this software to perform all necessary triangulation operations. A display of polygon triangulation can be seen in \fref{fig:triangulatuon}

\begin{figure}
	\centering
	\input{pictures/triangulation.tex}
	\caption{\small Triangulation of the polygon seen in \fref{fig:minAreaRect}.}
	\label{fig:triangulatuon}
\end{figure}

\subsubsection{AForge.NET}
\link{http://www.aforgenet.com}{AForge.NET} is a C\# framework containing implementations of various A.I. related algorithms and applications such as image processing, neural networks, machine learning and robotics. We used a very small part of this framework to communicate with the LEGO NXT Brick. AForge.NET handles the bluetooth connection with the brick and supplies convenient methods to issue motor commands.

\subsubsection{Native NXT Firmware}
LEGO NXT Bricks come with LEGO's own firmware. Since AForge.NET was built with this in mind, we did not have to flash the brick with a different type of firmware.

\subsubsection{Algorithms}
Next to libraries, our project incorporates many code snippets that we found on the internet. Most if not all of these are specific mathematical or geometric operations (e.g. the angle between two vectors or the centroid of a polygon). In our code, all of the instances where we copy-pasted a third-party snippet contain a reference to the source. In total there are 12 such references.

\section{Design and Implementation}
Our program is set up in a very linear way and divided up into clearly separated modules. First of all, a subscription is made to the camera such that, each time a new image is captured, a chain of functionality is initiated that is known as the analysis pipeline. The course of this pipeline is illustrated in \fref{fig:programOverview}. The pipeline analyses the image and culminates into a single location that the robot is meant to move towards as a first step in reaching the goal.

Meanwhile, a connection to the robot was established. At the end of the analysis pipeline, the resulting location is presented to the robot control system, which determines the precise motor commands to sent to the robot, in order to reach the location.

All of the steps in the analysis and control pipeline communicate with two further components: the interface and the Constants. The interface are a couple of windows where intermediate results of calculations are displayed for debugging purposes, and by which the user interact with the program (e.g. during colour calibration). The Constants is the collection of values that modify the behaviour of the program, such as the calibrated colours and the dimensions of the robot.

\begin{figure}
	\begin{center}
	\input{pictures/programOverview.tex}
	\end{center}
	\caption{\small Program design overview.}
	\label{fig:programOverview}
\end{figure}

In many steps of the analysis pipeline, the concept of 'object types' is important. These represent the different objects that can be detected by the program. Each of these types requires a unique colour. The object types relevant to the final project are listed below (there are more object types present in the code, due to plans for extensions. See \sref{sec:multiagent}).

\begin{description}
  \item[Wall]
  \item[Goal]
  \item[Robot] A generic square that is placed on all robots
  \item[TransportRobot] A square placed specifically on the transport robot (the only robot in use in the final design)
\end{description}

--- this list is ugly ---

\subsection{Camera input}
We request the camera to provide us with images with a size of 1024 by 768 pixels. The higher the resolution, the lower the frame rate. We aimed for an analysis rate of three frames per second, and the chosen resolution enables the camera to provide images at three frames per second or higher. Each time the camera has captured an image, it activates the analysis pipeline, resulting (in most cases) in a new command being sent to the robot.

\subsection{Image Pre-processing}
The Logitech camera software enables us to apply a number of pre-processing modifications to the input image. The most important of these were the zoom, focus distance, exposure, gain, brightness, contrast, colour intensity and white balance of the image. Most of these needed to be manually re-calibrated whenever there was a change in the physical environment.

% picture of exposed settings: http://tdcat.com/wp-content/uploads/2013/03/makemostofskype_c920settings.png

\subsection{Colour Calibration}
Obviously, an important aspect of the analysis is knowing which colours signify which objects in the world. For the program to learn this, the user needs to calibrate the colour-object associations. The program then uses this information for all subsequent operations until the user performs a new calibration.

\subsubsection{Calibration Interface}
Calibration begins with choosing an object type to calibrate the colour of. The live pre-processed image is presented, and the user can click on it to indicate spots that should be seen as the colour of the chosen object type. In these spots, dots appear to indicate that the spot was clicked. These dots are of a fixed size which is of importance, as all pixels that are covered by the spot are seen as belonging to the chosen object type. A snapshot of the calibration process can be seen in \fref{fig:calibrationSpots}.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{pictures/calibrationSpots.png}
	\caption{\small Multiple selected spots for calibrating the goal colour.}
	\label{fig:calibrationSpots}
\end{figure}

Multiple spots can be clicked to cover a particular object as much as possible, or to cover multiple objects of the same type.

When the user has clicked all spots belonging to a certain object type (or as much as they deem sufficient to calibrate the colour), they can click a button to indicate they are finished. All pixels that were covered by dots are now used as calibration data in the calibration calculation (see below), and the results of this calculation are associated with the chosen object type. The user can now choose the next object type to calibrate.

Once all object types have been calibrated, the user can choose to save the set of calibrated values to a file. This file can then be loaded at a later time, sparing the user the effort of calibrating the values over again. Of course this is only a valid shortcut if the user is running the program under the same physical conditions as when saving the file. Otherwise, the colours of objects, as perceived by the webcam, may have changed.

\subsubsection{Calibration Calculation}

The calibration calculation, for a certain object type $o$, takes as input a collection of colours $C_o$ (the colours of those pixels that were covered with spots by the user) with each colour $c_{oi} = [r_{oi},g_{oi},b_{oi}]$ (for the red, green and blue channels). From these are distilled two pieces of information that form the calibration profile for that particular object type.

The first piece of information is $\overline{C_o}$, the average colour of the pixels, seen as the base colour of the object type.
 
\[
\overline{C_o} = [ \overline{r_o},\overline{g_o},\overline{b_o} ]
\]

The second piece is $T_o$, known as the threshold value of the object type. It is the maximum distance from the average, over all colours in the set.

\[
T_o = \max_{i} \dist( \overline{C_o}, C_{oi} )
\]

where

\[
\dist(c_i,c_j) = \vert r_{i}-r_{j} \vert + \vert g_{i}-g_{j} \vert + \vert b_{i}-b_{j} \vert 
\]

This last equation demonstrates that we use a component-wise colour distance function. We considered using a Euclidean distance function, but this proved less optimizable (during Colour Extraction) and we also didn't find a good reason to interpret colours as a three-dimensional space. In any case, the component-wise distance function serves us well.

\subsection{Colour Extraction}
At this point in the pipeline the program has received an image from the webcam and has access to calibration data detailing which colours are associated with which objects, and analysis can begin. The first step is to create masks of the input image, indicating which pixels fall within the thresholds of which objects. One such mask is created separately for each object type. A contiguous region of masked area is known as a blob. An example of extracted colour can be seen in \fref{fig:colourmask}.

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\includegraphics[width=\columnwidth]{pictures/colourmask1.png}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\includegraphics[width=\columnwidth]{pictures/colourmask2.png}
	\end{subfigure}
	\caption{Section of input image and corresponding colour mask for the purple robot marker.}
	\label{fig:colourmask}
\end{figure}

\subsubsection{Colour Masking Algorithm}
To create a colour mask of a certain object type $o$ in a given image $P$ consisting of pixels with colours $p_i$, the average colour $\overline{C_o}$ and threshold value $T_o$ of the object type are retrieved. The mask $M$ then is an image of the same size and shape as $P$, with pixels of colours $m_i$, where

\[
m_i =
	\begin{cases}
	1 & \mbox{if } \dist( \overline{C_o}, p_i, ) < T_o \cdot \mu \\
	0 & \mbox{otherwise}
	\end{cases}
\]
$\mu$ here is what's known as the 'threshold multiplier', a constant value that allows for more or less leniency when matching colours. $\mu = 2$ would mean that the distance of a colour to the average may be twice as large as the distance found during calibration, and still be masked as belonging to the object type. Through experimentation we settled on $\mu = 1.5$.

\subsubsection{Optimization}
Although this report avoids code-specific details, special efforts went into optimizing the colour masking function, which are worth pointing out. This function needed to be quite fast, as it needs to run each frame, for all object types, over all pixels in the input stream. There were several optimizations made to aid in this.

First, as much of the required information is declared and initialized before the pixels are looped over, so that it need not be extracted over and over for each pixel.

Then, there is an 'early-out' mechanism, based on the fact that colour distance is the sum of the component distances. If a single component's distance (or the sum of two components' distances) is larger than the allowed colour distance, it is clear that the total distance is too large.

Since calculation of the absolute value of a number is used in the repeated colour distance calculation, it is an intensively used function and should be as fast as possible. Because colour component values can be represented by 8 bits, we implemented an absolute value function that works with 16-bit 'short integer' variables rather than the existing 32-bit function. We use a function invented and patented by Volkonsky in 1997 \cite{volkonsky2000apparatus}.

Finally, we utilize the .NET Parallel class to parallelize the loops over pixels and object type. This creates threads for each iteration of the different loops, causing the many iterations to be executed mostly simultaneously. This means that not all operations may be executed in order, but in our case this causes no problems, as pixels and object types are evaluated independently from each other.

\subsection{Contour Detection}
To be able to match an object in an image, we need to find a representation of the image such that we can assess whether it matches our ideas about the object. We do this by taking the masks created in the previous step, and converting them to polygons; vector representations of the contours of the masks. Thanks to EmguCV, this takes only a single line of code. An example of detected contours can be seen in \fref{fig:retrievedContours}.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{pictures/retrievedContours.png}
	\caption{\small Example of retrieved wall contours.}
	\label{fig:retrievedContours}
\end{figure}

\subsection{Object Matching}
Having retrieved contours describing the matched shapes in the previous step, we now need to find out which contours actually correspond to the objects we are looking for, and which are flukes caused by fluctuating physical conditions. To do this, we try to match the contours to our ideas about the object types we are looking for.

Since we know which from which object type a particular set of contours was retrieved, we can look at contours to find specific properties that match our idea about the object type. If a contour matches the properties of the object type we are looking for, we accept it as being of that object type. Most contours we find are discarded as they are found not to match the object type. An example of matched objects can be see in \fref{fig:matchedObjects}.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{pictures/matchedObjects.png}
	\caption{\small Objects matched from contours including those in \fref{fig:retrievedContours}.}
	\label{fig:matchedObjects}
\end{figure}

There are practically two different ways of matching contours to object types: rectangles and arbitrary shapes.

\subsubsection{Matching Rectangles}
\label{sec:matchingRectangles}
Many of the object types we are looking for are rectangular in shape. To check whether a particular contour is rectangular, we start by computing the convex hull of the contour. We do this because we know rectangles are convex, and any shape that matches our idea of a rectangle will be convex.

The points that make up the convex hull are then consolidated. By this we mean that points that are within a certain distance from each other are merged, and points that form an angle close to 180\degree with their neighbours are removed. Both of these modifications serve to clean up points that add little definition to the shape. An example of the consolidation process can be seen in \fref{fig:hullConsolidation}.

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\includegraphics[width=\columnwidth]{pictures/hullConsolidation1.png}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\includegraphics[width=\columnwidth]{pictures/hullConsolidation2.png}
	\end{subfigure}
	\caption{A convex hull and its consolidated counterpart.}
	\label{fig:hullConsolidation}
\end{figure}

We then check whether the resulting shape has at least four points. We also check whether the area of the hull is above a certain value, thereby ignoring tiny variations in light and surface colour.

If these are true, we accept the contour as being close enough to rectangular in shape, and as being an instance of the particular object type we are looking for. 

\subsubsection{Matching Arbitrary Shapes}
If an object type's shape is not rectangular, then it is arbitrary (walls and the goal). Arbitrary shapes are matched by simplifying the contour (EmguCV functionality) and checking whether the contour has an area above a certain value (again, hereby ignoring small mistakes in the mask). If so, the contour is accepted as the object type in question.

\subsubsection{Matching Robots}
---
about how the two squares are used to calculate position and angle of robot (taking into account rotation point offset). or should this go in physical considerations? with image
---

\fref{fig:matchingRobots}

\begin{figure}
	\centering
	\input{pictures/matchingRobots.tex}
	\caption{\small Visualization of the robot-matching process, with the robot marker in green, the transport robot marker in yellow, the center of those the white cross, the direction of the robot shown by the arrow, and the actual center of rotation the black cross. Light grey is the robot body, with the motorized wheels in dark grey.}
	\label{fig:matchingRobots}
\end{figure}

\subsection{Navigation Mesh}
Now that we have complete knowledge of all relevant objects in the world, including the location of the robot and the goal and the shapes of the walls, we can start thinking about how the robot is going to reach the goal. It seems obvious that we will need to use some kind of pathfinding algorithm.

\subsubsection{Initial Idea: Grid}
At first, we considered dividing the world up into a grid, with each grid point either inside a wall or not inside a wall, then using the standard algorithm of pathfinding through this grid using A*. However, we foresaw several potential problems with this approach. For starters, to determine which points were inside walls, they would each need to be checked for inclusion in all wall objects. Likely a time-consuming task. Especially since the number of points would probably be quite large, to create a high enough resolution. Such a resolution would be needed to allow for smaller passages through obstacles to be captured. Also, a grid-based pathing approach would limit us to movement in horizontal, vertical and diagonal directions, or require us to find tricks to expand those directions. Finally, running A* on such a large amount of points would probably be another performance issue.

\subsubsection{NavMesh Definition}
We found an alternative in navigation meshes (or NavMeshes): a technique gleaned from use in video game AI. The core concept of navigation meshes is to divide the world up into interconnected areas of arbitrary shape, forming a network. That network can then be used to perform the (initial) steps of pathfinding on, for example A*. An example of a NavMesh can be seen in \fref{fig:navmeshExample}.

\begin{figure}
	\centering
	\input{pictures/navmeshExample.tex}
	\caption{\small Generic example of a NavMesh with walls in black, navigation area boundaries in green.}
	\label{fig:navmeshExample}
\end{figure}

For our specific case, we decided to add several constrains to the NavMesh geometry. Most importantly, all individual areas in the NavMesh are convex shapes. This ensures that when the robot is in a particular area, it can always move freely to any other point in the area. Effectively, this means that the only pathfinding we need to do is between the edges that connect areas, as all movement between those is unobstructed. This does not however take into account the width of the vehicle as it moves close to the edge of an area, which is discussed in \sref{sec:pathrefinement}. 

\subsubsection{Construction}
There are several steps involved in the construction of the NavMesh. First, the negative space left by the walls is triangulated. The triangles are then consolidated into a network of polygons. Finally, the small-passage-avoidance algorithm ensures that narrow passages are removed from the network.

\paragraph{Triangulation}
In order to start building the NavMesh, we triangulate the negative space left by the wall objects. Technically, we create a large polygon that covers the whole world, and then add the walls into that polygon as holes. Triangulating that polygon using Triangle.NET results in what can be seen as a rough version of the NavMesh. Although the triangles could already at this point be used for pathfinding, the network that they form is unnecessarily large. An example of negative space triangulation can be seen in \fref{fig:triangulation}.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{pictures/triangulation.png}
	\caption{The negative space around some walls, triangulated.}
	\label{fig:triangulation}
\end{figure}

\paragraph{Consolidation}
We now start merging neighbouring triangles into larger polygons using a process we call consolidation (not to be confused with intra-polygon consolidation discussed in \sref{sec:matchingRectangles}). When two polygons are merged, they form a new polygon with the same shape as the two initial polygons combined. This new polygon takes the place of the two initial polygons in the network. An example of polygon consolidation can be seen in \fref{fig:consolidation}.

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\input{pictures/consolidation1.tex}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\input{pictures/consolidation2.tex}
	\end{subfigure}
	\caption{A pair of polygons on the left and their consolidated version on the right. All of these polygons are convex.}
	\label{fig:consolidation}
\end{figure}

To decide which polygons to merge, all combinations between two neighbouring polygons are evaluated. If the polygons would form a convex shape when merged, then so they should be.

There is the important question of the order in which to merge polygons, since the merger of polygons $A$ and $B$ will exclude the merger of polygons $B$ and $C$. Since the aim is to create as few and as large areas as possible, the policy is to check out smaller polygons first. Since we don't want to use two smaller polygons to create a new small area with no consolidable neighbours, the policy is to look for consolidations with larger neighbours first. These policies make for a NavMesh with mostly large areas.

This process of consolidation is executed iteratively. That is, consolidated polygons are again checked for new consolidation possibilities. When no more valid combinations can be found, consolidation is completed. An example of the result of the consolidation process can be seen in \fref{fig:consolidatedTriangles}.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{pictures/consolidatedTriangles.png}
	\caption{The triangles from \fref{fig:triangulation}, consolidated into polygons.}
	\label{fig:consolidatedTriangles}
\end{figure}

\paragraph{Avoiding small passages}
Since our robot is not a point, but has a width, we want to avoid producing a path between two walls that the robot would not physically be able to fit through. Given the workings of our NavMesh, all edges that connect two areas start and end on walls (specifically, corners of walls). This makes for an easy way to approach the problem of small passage avoidance. Still, finding a method that worked correctly in all situations was a challenge.

Perhaps the most obvious thing to do, in order to find connections between areas that are too small to pass through, is to check whether the edge that connects the areas is longer than the width of the vehicle. This turns out to work fine in some cases, but not in others, especially when the edge is at a large angle compared to neighbouring edges. This is demonstrated in \fref{fig:edgeLengthMethod}.

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\input{pictures/edgeLengthMethod1.tex}
	\subcaption{Edge-length method working correctly (robot in green, walls in black, avoided edges in red)}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\input{pictures/edgeLengthMethod2.tex}
	\subcaption{Edge-length method working incorrectly: the edges are not avoided even though the passage is too narrow.}
	\end{subfigure}
	\caption{Edge-length method of avoiding small passages.}
	\label{fig:edgeLengthMethod}
\end{figure}

When confronted with the previous problem, we were quick to suggest that a solution might be to evaluate the size of the whole polygon -- looking at some measurement of its width to determine whether the robot would be able to fit through it. This would solve the above problem, but would falsely identify some other polygons as being too narrow, as can be seen in \fref{fig:polygonWidthMethod}.

\begin{figure}
	\centering
	\input{pictures/polygonWidthMethod.tex}
	\caption{A polygon incorrectly identified as impassable by the width-based method.}
	\label{fig:polygonWidthMethod}
\end{figure}

After some more considerations, we found a reliable method $A_e$ of determining whether an edge $e$ connecting two areas should be avoided due to it being too small to pass through.
\begin{align*}
A_e = &\ \forall p,q \in P_e, \exists f \in E_q : \\
&\ p \neq q \land e \neq f \land \dist(p,\proj(p,e)) < \psi
\end{align*}

where
\begin{align*}
P_e &= \textrm{the set of points defining edge $e$} \\
E_p &= \textrm{the set of edges partially defined by point $p$} \\
\proj(p,e) &= \textrm{the projection of point $p$ onto edge $e$} \\
\dist(p,q) &= \textrm{the distance between two points $p$ and $q$} \\
\psi &= \textrm{the width of the robot}
\end{align*}

In words: an edge must be avoided if, for both points that define the edge, there exists another edge connected to the other point, such that the distance of the projection of the point onto that edge is smaller than the width of the robot. A visualization of this function is shown in \fref{fig:projectionMethod}.

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\input{pictures/projectionMethod1.tex}
	\subcaption{}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\input{pictures/projectionMethod2.tex}
	\subcaption{}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\input{pictures/projectionMethod3.tex}
	\subcaption{}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\columnwidth}
	\centering
	\input{pictures/projectionMethod4.tex}
		\subcaption{}
	\end{subfigure}
	\caption{Purple projection of one point on the green edge onto the neighbouring blue edge, solving all previously shown problems. In figures (a) through (c), the projection distance is smaller than the robot width, and the green edge is avoided. In figure (d), the opposite is true.}
	\label{fig:projectionMethod}
\end{figure}

This method is completely successful at determining whether edges should be avoided or not. When a narrow edge is found, the connectivity that it forms between two polygons is removed from the NavMesh.

\subsection{Pathing}
With the NavMesh area-network in place, we can find a path from the robot to the goal. After embedding the start and end points into the NavMesh, A* is used to find a path between them, after which proximal points are removed from the path. The result of all this is a single point; the location that the robot presently needs to travel towards. The rest of the path is discarded, as we expect the world may change with time, rendering the previously computed path irrelevant. An example of a computed path can be found in \fref{fig:computedPath}.

\subsubsection{Embedding points}
We need to insert the start and end points of our desired path (the robot and goal locations) into the NavMesh. To do this for either of those points, we find the polygon in the NavMesh that contains the point. We then create connections between the point and the edges of that polygon. The result of this process can be seen in \fref{fig:finalNavMesh}.

\begin{figure}
	\centering
	\input{pictures/finalNavMesh.tex}
	\caption{Start and end points embedded into NavMesh, showing edges in grey and connections in blue.}
	\label{fig:finalNavMesh}
\end{figure}

\subsubsection{A* / Dijkstra}

Now that all the ingredients are in place, we run our implementation of A* to find a path from start to finish. It should be noted that we never bothered to implement a heuristic estimate function, since our search space is so small (thanks to the NavMesh) that its contribution to performance would be negligible. Because of this, we are really just running Dijkstra's algorithm.

A call to our A* implementation is structured as follows:

\[
\textrm{A*}(\alpha,\beta,N,D,H) = P
\]

where
\begin{align*}
\alpha =&\ \textrm{the start state} \\
\beta =&\ \textrm{the end state} \\
N =&\ \textrm{a function of the form $N(s)$} \\
&\ \textrm{to retrieve the child states of state $s$} \\
D =&\ \textrm{a function of the form $D(s,t)$} \\
&\ \textrm{to retrieve the cost of traversing from state $s$ to state $t$} \\
H =&\ \textrm{a function of the form $H(s)$} \\
&\ \textrm{to retrieve the heuristc estimate for state $s$} \\
P =&\ \textrm{a sequence of states that form the path}
\end{align*}

Any data type can be used to represent states by defining the above functions. In a (semi) functional language such as C\# this is easily achieved through the use of lambda expressions.

\subsubsection{Proximal node removal}
Having found the path we wish to travel, we will remove all nodes from the front of the path that the robot is currently close to. This way, we determine when the robot has reached a point in a path. Otherwise, it would practically never reach the point, due to the continuous nature of the locations of the point and the robot. The function $\Pi$ to clean the path of proximal nodes can be written recursively.

\[
\Pi(P) =
	\begin{cases}
	P & \textrm{if } \dist(R,P_0) > \Delta \\
	\Pi( P \setminus P_0 ) & \textrm{otherwise}
	\end{cases}
\]

where

\begin{align*}
P &= \textrm{a sequence of points that form the path} \\
\dist(p,q) &= \textrm{the distance between points $p$ and $q$} \\
R &= \textrm{the current location of the robot} \\
\Delta &= \textrm{distance at which robot is said to reach a point}
\end{align*}

What remains is a sequence of nodes starting with the first location that the robot needs to move towards. This location is handed over to the robot control system.

\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{pictures/computedPath.png}
	\caption{A path found through the NavMesh from \fref{fig:consolidatedTriangles} (note: only partial NavMesh shown in previous images). The yellow dot is the current point the robot should move towards.}
	\label{fig:computedPath}
\end{figure}

\subsection{Communication}
AForge.NET supplies various convenience methods to communicate with LEGO NXT Bricks through bluetooth. A serial port connection is established between the computer and the brick, and the framework converts high-level messages such as "set motor A's speed to 50" to a correctly formatted bytestring.

--- robot control window? ---

\subsection{Control}
Once we have established a connection between the computer and the robot, we can start executing the plan we created. In order to achieve this, we need to know a couple of things:

\begin{itemize}
	\item[-] Robot's position
	\item[-] Robot's orientation
	\item[-] Desired position
\end{itemize}

We find all of these in previous steps in our processing pipeline. Next, we define a couple of values:

\begin{description}
\item[Destination offset angle ($\delta_{dest}$)] The difference between the robot's orientation and the angle of the robot with the destination.
\item[Maximum offset margin ($\delta_{max}$)] The maximum offset with the destination that still passes as "the correct direction".
\end{description}

To decide what the robot will actually do (turn or drive forward), we compare these values. If $\delta_{dest} > \delta_{max}$, the robot turns according to the following:

\[
\textrm{direction} =
\begin{cases}

\textrm{left} & \textrm{if } \delta_{dest} < 0 \\
\textrm{right} & \textrm{otherwise}
\end{cases}
\]

We added some flexibility in our code to avoid the robot flipping from left to right constantly when $\lvert \delta_{dest}\rvert$ was close to $180\degree$.
If $\delta_{dest} \leq \delta_{max}$ it's easy; we just have the robot drive forward.

\subsection{Robustness}
In each of the previous steps, we have always assumed that certain information was present. For example, we assumed that a path was possible between the robot and the goal, while this may not always be the case, depending on the layout of the walls. This may raise the question, what happens in such cases? In fact, when the program encounters such a problem, or even an exception raised at some point in the code, the current run through the analysis pipeline is aborted and the program waits for a new input image to arrive.

At such a time, the robot simply continues executing the last action it received from the analysis pipeline. The assumption being that some subsequent input image will have different conditions such that the analysis pipeline is once again able to run fully, resulting in updated information. This approach might seem dangerous, but actually works out quite well, providing that the user knows about certain restrictions when manipulating the world (such as the need for there to be a path to the goal).

\section{Physical Considerations}
Problems, choices, etc. to do with physical things.

robot construction
world construction
lamp
calibration sheets

CASPAR

\section{Legacy Code and Unimplemented Features}
Any features that we wanted to add but didn't, code that we worked on but dropped, thoughts on these...

what should be the order of these things?

\subsection{Multi-Agent Interaction}
\label{sec:multiagent}

\subsection{Sockets}
ERIK

\subsection{Microsoft Robotics Studio}
ERIK/CHRIS

\subsection{Smart World Model}
KOEN/MAREIN

\subsection{Path Refinement}
\label{sec:pathrefinement}
MAREIN

\subsection{HSV Color Model}
KOEN/MAREIN

\subsection{Smaller Robots}

\subsection{Scale Calibration}

\subsection{Robot Endearment}
robot celebrates reaching goal, gets confused when there's no path, angry when goal changing often without reaching

\subsection{EmguCV Data Formats}
\label{sec:emgucvFormats}
As described, we make use of the data formats that EmguCV brings to the table, a necessity when using the library's functionalities. There are two problems with this, with an eye on performance.

First, the EmguCV formats are not optimized for our purposes. For example, when we are using EmguCV images to store blob information, which are really binary bitmaps. The EmguCV format dictates that the image is at least a grayscale image, which is unnecessary for our purpose of storing binary values. Things such as these may contribute negatively to performance.

Second, we make use of Windows Presentation Foundation (WPF), which is not directly compatible with the EmguCV formats. WPF is a system that facilitates the creation of user interfaces for .NET applications. We use it, among other things, to display results at various stages of the analysis process. Since these results are often in EmguCV formats, they need to be converted before being handled by WPF. This takes time and probably has a large negative impact on performance. Note that one optimization we apply is to only convert the relevant data that the user is looking at, at a particular moment.

We had vague plans of taking the EmguCV formats out of our own code and only converting from and to it when interacting with the EmguCV functionality (which is actually only in very few places). This should provide a boost in performance, and create cleaner code, with only our own representations directly visible, and none of EmguCV's.

\section{Demo Evaluation}

\section{Team}

\subsection{Workload Distribution}
Individual description/list of tasks completed 

\subsection{Remarks}
Individual remarks on the project.

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
